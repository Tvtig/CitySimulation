Steps,Policy/Entropy,Policy/Extrinsic Value Estimate,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
50000,1.4136254,-0.54394865,815.311475409836,-167.98360655737704,-167.98360655737704,9.21293,0.023449894,0.00028456698,0.19485566,0.0047432967,1.0
100000,1.4129437,-3.7337239,578.7764705882353,-126.67058823529412,-126.67058823529412,8.047416,0.026041353,0.00025684942,0.18561646,0.0042822617,1.0
150000,1.4099147,-5.598123,901.4107142857143,-164.375,-164.375,5.08828,0.019575942,0.0002260528,0.17535092,0.0037700112,1.0
200000,1.4054713,-5.470712,901.4909090909091,-97.69090909090909,-97.69090909090909,2.0674062,0.023697332,0.00019525409,0.16508469,0.0032577254,1.0
250000,1.4031909,-5.440268,803.0322580645161,-70.61290322580645,-70.61290322580645,1.195863,0.025350874,0.00016449044,0.1548301,0.002746023,1.0
300000,1.4013801,-5.722776,811.5806451612904,-66.33870967741936,-66.33870967741936,1.0205985,0.023532312,0.00013366734,0.14455576,0.0022333325,1.0
350000,1.3997416,-5.8154025,778.936507936508,-56.42857142857143,-56.42857142857143,0.9705082,0.021957206,0.0001028492,0.13428304,0.0017207237,1.0
400000,1.397936,-6.0363946,764.4776119402985,-52.343283582089555,-52.343283582089555,0.9286436,0.023466207,7.5121585e-05,0.1250405,0.0012595211,1.0
450000,1.3967359,-6.5432067,879.7142857142857,-63.05357142857143,-63.05357142857143,0.87990206,0.022475094,4.7391088e-05,0.115797006,0.0007982703,1.0
500000,1.3962882,-6.8861485,850.2372881355932,-58.42372881355932,-58.42372881355932,0.9252675,0.025179539,1.659501e-05,0.10553165,0.00028602878,1.0
